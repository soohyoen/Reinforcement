{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc80ccb",
   "metadata": {},
   "source": [
    "#### 기존의 Q-learning은 state-action(s,a)에 해당하는 Q-value인 Q(s,a)를 테이블 형식으로 저장하여 학습함\n",
    "#### 이러한 방식은 state space와 action space가 커지면 모든 Q-value를 저장해야해서 memory와 exploration time 이슈가 생김\n",
    "#### 딥러닝을 이용하여 Q-table.에 해당하는 Q-function을 비선형함수로 근사시켜 모든 S-A에 대한 Q-value값을 찾거나 저장할 필요를 없앰\n",
    "#### 이때 딥러닝으로 근사한 Q-function의 weight parameter들은 일반적으로 세타로 표현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fc62677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72d45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "MAX_EP = 25000\n",
    "\n",
    "REWARD_ACC = list()\n",
    "LOSS_ACC = list()\n",
    "\n",
    "# pytorch에서는 random seed를 고정하기 위한 함수로 manual_seed를 제공함.\n",
    "# random값을 고정하기 위해 수동으로 설정하는 것\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "337835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 숫자 혹은 array을 넣으면 요소들의 곱을 얻을 수 있음.\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        # torch 타입에서 tensor로 타입 변환\n",
    "        state_t = torch.as_tensor(state, dtype = torch.float32)\n",
    "        \n",
    "        # 1인 차원을 추가시켜줌(Q_values 는 두가지 벨류를 내뱉음 -> left or right)\n",
    "        q_values = self.forward(state_t.unsqueeze(0))\n",
    "        \n",
    "        # 최대값에 해당되는 지수를 구한다\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        \n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7100e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "episode_reward = 0.0\n",
    "episode = 0\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "net = Network(env)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52ac1ad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0\n",
      "Avg Reward 0.0\n",
      "Loss tensor(0.5948, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 1000\n",
      "Avg Reward 18.92156862745098\n",
      "Loss tensor(0.5998, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 2000\n",
      "Avg Reward 21.063157894736843\n",
      "Loss tensor(5.3067, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 3000\n",
      "Avg Reward 20.48\n",
      "Loss tensor(0.3970, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 4000\n",
      "Avg Reward 17.7\n",
      "Loss tensor(5.6945, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 5000\n",
      "Avg Reward 16.39\n",
      "Loss tensor(0.2447, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 6000\n",
      "Avg Reward 15.43\n",
      "Loss tensor(0.4341, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 7000\n",
      "Avg Reward 16.23\n",
      "Loss tensor(1.0131, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 8000\n",
      "Avg Reward 12.43\n",
      "Loss tensor(0.4443, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 9000\n",
      "Avg Reward 12.37\n",
      "Loss tensor(0.2295, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 10000\n",
      "Avg Reward 10.62\n",
      "Loss tensor(0.2475, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 11000\n",
      "Avg Reward 9.54\n",
      "Loss tensor(0.0647, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 12000\n",
      "Avg Reward 9.62\n",
      "Loss tensor(0.3071, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 13000\n",
      "Avg Reward 9.48\n",
      "Loss tensor(0.0918, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 14000\n",
      "Avg Reward 9.52\n",
      "Loss tensor(0.0583, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 15000\n",
      "Avg Reward 9.52\n",
      "Loss tensor(0.0762, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 16000\n",
      "Avg Reward 9.71\n",
      "Loss tensor(0.0495, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 17000\n",
      "Avg Reward 9.39\n",
      "Loss tensor(0.0005, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 18000\n",
      "Avg Reward 9.78\n",
      "Loss tensor(0.0024, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 19000\n",
      "Avg Reward 9.81\n",
      "Loss tensor(0.0024, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 20000\n",
      "Avg Reward 9.5\n",
      "Loss tensor(0.0079, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 21000\n",
      "Avg Reward 9.84\n",
      "Loss tensor(0.0598, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 22000\n",
      "Avg Reward 9.74\n",
      "Loss tensor(0.0203, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 23000\n",
      "Avg Reward 9.56\n",
      "Loss tensor(0.0006, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 24000\n",
      "Avg Reward 9.56\n",
      "Loss tensor(0.0003, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "Step 25000\n",
      "Avg Reward 9.9\n",
      "Loss tensor(0.0082, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    \n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    \n",
    "    random_sample = random.random()\n",
    "    \n",
    "    # random_sample의 값이 epsilon보다 작으면, random한 action을 취한다.\n",
    "    if random_sample <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    else:\n",
    "        action = net.act(state)\n",
    "        \n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    state = new_state\n",
    "    \n",
    "    episode_reward = episode_reward + reward\n",
    "    \n",
    "    if done:\n",
    "        # env reset\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reward_buffer에 reward를 더해준다\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "    state_t = torch.as_tensor(state, dtype = torch.float32)\n",
    "    action_t = torch.as_tensor(action, dtype=torch.int64).unsqueeze(-1)\n",
    "    reward_t = torch.as_tensor(reward, dtype = torch.float32).unsqueeze(-1)\n",
    "    done_t = torch.as_tensor(done, dtype = torch.float32).unsqueeze(-1)\n",
    "    new_state_t = torch.as_tensor(new_state, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = net.forward(new_state_t)\n",
    "    max_target_q_values = target_q_values.max(dim=0, keepdim=True)[0]\n",
    "    targets = reward_t + GAMMA * (1-done_t) * max_target_q_values\n",
    "    \n",
    "    q_values = net.forward(state_t)\n",
    "    action_q_values = torch.gather(input=q_values, dim=0, index=action_t)\n",
    "    \n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "    \n",
    "    # 0으로 설정하는 대신 등급을 없음으로 설정함\n",
    "    optimizer.zero_grad()\n",
    "    # 현재 텐서 wrt 그래프 리프의 기울기를 계산함\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Reward', np.mean(reward_buffer))\n",
    "        print('Loss',loss)\n",
    "        REWARD_ACC.append(np.mean(reward_buffer))\n",
    "        LOSS_ACC.append(loss.item())\n",
    "        \n",
    "    if step == MAX_EP:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DQN.txt','w') as f:\n",
    "    f.write(str(REWARD_ACC))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(LOSS_ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693a75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein_study",
   "language": "python",
   "name": "rein_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
