{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062a8479",
   "metadata": {},
   "source": [
    "## Cart-Pole 예제에서 Expected SARSA 에이전트 학습\n",
    "### x : Cart의 가로상의 위치\n",
    "### θ : Pole의 각도\n",
    "### dx/dt : Cart의 속도\n",
    "### dθ/dt : θ의 각속도\n",
    "###### 세타가 15도 이상이 되거나, 원점으로부터의 x의 거리가 2.4이상이 되었을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b998d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42faf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep SARSA 에이전트 클래스를 만들어줌\n",
    "# 인공신경망의 레이어 수는 세개 각각 32개의 노드를 사용한다.\n",
    "# 옵티마이저는 Adam을 사용\n",
    "# 활성화 함수 : 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수\n",
    "# ReLU → 입력값이 0보다 작으면 0을 0보다 크면 입력값 그대로 출력\n",
    "\n",
    "class DeepSARSA:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_decay = .99995\n",
    "        self.epsilon_min = 0.01\n",
    "          \n",
    "        self.model =nn.Sequential(\n",
    "            nn.Linear(self.num_states, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.num_actions)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.alpha)\n",
    "        \n",
    "    # 현재상태, 현재행동, 보상, 다음상태, 다음행동 을가지고 시간차 학습\n",
    "    # 에피소드가 끝나는 시점에는 미래의 값을 고려하지 않고 학습함\n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \n",
    "        self.decrease_epsilon()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        q_value = self.model(state)[action]\n",
    "        next_q_value = self.model(next_state)[next_action].detach()\n",
    "        \n",
    "        q_target = reward + (1 - int(done)) * self.gamma * next_q_value\n",
    "        q_error = (q_target - q_value) ** 2\n",
    "        \n",
    "        q_error.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return q_error.item()\n",
    "    \n",
    "    def decrease_epsilon(self):      \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def act(self, state):        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            q_values = self.model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b174271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00431391,  0.02705465, -0.01444569, -0.03511632])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "env = gym.make('CartPole-v1')\n",
    "env = wrappers.Monitor(env, \"./video\", force=True)\n",
    "observation = env.reset()\n",
    "agent = DeepSARSA(4,2)\n",
    "\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a9bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soohyoen/anaconda3/envs/rein_study/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, eps: 0.991, loss : 1.6, rewards: 16.0\n",
      "episode : 20, eps: 0.983, loss : 9.5, rewards: 19.0\n",
      "episode : 30, eps: 0.971, loss : 8.1, rewards: 19.0\n",
      "episode : 40, eps: 0.962, loss : 11.9, rewards: 23.0\n",
      "episode : 50, eps: 0.953, loss : 5.5, rewards: 32.0\n",
      "episode : 60, eps: 0.941, loss : 8.7, rewards: 45.0\n",
      "episode : 70, eps: 0.933, loss : 30.1, rewards: 10.0\n",
      "episode : 80, eps: 0.920, loss : 13.8, rewards: 17.0\n",
      "episode : 90, eps: 0.911, loss : 11.3, rewards: 20.0\n",
      "episode : 100, eps: 0.899, loss : 9.7, rewards: 20.0\n",
      "episode : 110, eps: 0.889, loss : 12.1, rewards: 11.0\n",
      "episode : 120, eps: 0.878, loss : 5.0, rewards: 30.0\n",
      "episode : 130, eps: 0.867, loss : 2.7, rewards: 34.0\n",
      "episode : 140, eps: 0.859, loss : 3.5, rewards: 10.0\n",
      "episode : 150, eps: 0.846, loss : 8.4, rewards: 28.0\n",
      "episode : 160, eps: 0.834, loss : 9.9, rewards: 21.0\n",
      "episode : 170, eps: 0.819, loss : 7.9, rewards: 48.0\n",
      "episode : 180, eps: 0.805, loss : 12.9, rewards: 27.0\n",
      "episode : 190, eps: 0.794, loss : 7.3, rewards: 15.0\n",
      "episode : 200, eps: 0.785, loss : 9.9, rewards: 16.0\n",
      "episode : 210, eps: 0.770, loss : 17.5, rewards: 18.0\n",
      "episode : 220, eps: 0.755, loss : 16.2, rewards: 26.0\n",
      "episode : 230, eps: 0.743, loss : 16.0, rewards: 23.0\n",
      "episode : 240, eps: 0.732, loss : 7.2, rewards: 23.0\n",
      "episode : 250, eps: 0.711, loss : 4.5, rewards: 47.0\n",
      "episode : 260, eps: 0.695, loss : 8.9, rewards: 27.0\n",
      "episode : 270, eps: 0.683, loss : 21.1, rewards: 22.0\n",
      "episode : 280, eps: 0.671, loss : 17.6, rewards: 21.0\n",
      "episode : 290, eps: 0.658, loss : 39.3, rewards: 10.0\n",
      "episode : 300, eps: 0.641, loss : 22.2, rewards: 35.0\n",
      "episode : 310, eps: 0.624, loss : 7.4, rewards: 98.0\n",
      "episode : 320, eps: 0.606, loss : 20.6, rewards: 18.0\n",
      "episode : 330, eps: 0.581, loss : 16.2, rewards: 71.0\n",
      "episode : 340, eps: 0.559, loss : 8.1, rewards: 141.0\n",
      "episode : 350, eps: 0.542, loss : 11.0, rewards: 153.0\n",
      "episode : 360, eps: 0.513, loss : 46.7, rewards: 61.0\n",
      "episode : 370, eps: 0.476, loss : 76.0, rewards: 32.0\n",
      "episode : 380, eps: 0.458, loss : 17.8, rewards: 132.0\n",
      "episode : 390, eps: 0.422, loss : 53.6, rewards: 69.0\n",
      "episode : 400, eps: 0.393, loss : 61.6, rewards: 35.0\n",
      "episode : 410, eps: 0.336, loss : 6.9, rewards: 500.0\n",
      "episode : 420, eps: 0.304, loss : 1.2, rewards: 117.0\n",
      "episode : 430, eps: 0.278, loss : 0.9, rewards: 140.0\n",
      "episode : 440, eps: 0.244, loss : 27.7, rewards: 317.0\n",
      "episode : 450, eps: 0.197, loss : 202.1, rewards: 31.0\n",
      "episode : 460, eps: 0.186, loss : 6.9, rewards: 261.0\n",
      "episode : 470, eps: 0.152, loss : 8.7, rewards: 402.0\n",
      "episode : 480, eps: 0.141, loss : 266.0, rewards: 11.0\n",
      "episode : 490, eps: 0.120, loss : 16.0, rewards: 247.0\n",
      "episode : 500, eps: 0.102, loss : 11.4, rewards: 309.0\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for ep in range(500):\n",
    "    done = False\n",
    "    obs = torch.FloatTensor(env.reset())\n",
    "    action = agent.act(obs)\n",
    "    \n",
    "    ep_rewards = 0\n",
    "    losses = []\n",
    "    while not done:\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_obs = torch.FloatTensor(next_obs)\n",
    "        \n",
    "        next_action = agent.act(next_obs)\n",
    "        \n",
    "        loss = agent.update(obs, action, reward, next_obs, next_action, done)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        ep_rewards += reward\n",
    "        obs = next_obs\n",
    "        action = next_action\n",
    "    rewards.append(ep_rewards)\n",
    "    ep_loss =sum(losses) / len(losses)\n",
    "    if (ep+1) % 10 == 0 :\n",
    "        print(\"episode : {}, eps: {:.3f}, loss : {:.1f}, rewards: {}\". format(ep+1, agent.epsilon,ep_loss, ep_rewards ))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9d89ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Deep SARSA_1.ipynb'   \u001b[0m\u001b[01;34mvideo\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028a6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein_study",
   "language": "python",
   "name": "rein_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
