# -*- coding: utf-8 -*-
"""Copy of _3 케라스(Keras) 기초부터 모델 생성까지.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1avNNyFvTe_FbnZqdA8zER3uNlZh7ABe-

# 케라스(Keras) 기초부터 모델 생성까지

<img src="https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png">

- 파이썬으로 작성된 고수준 신경망 API로 TensorFlow, CNTK, 혹은 Theano와 함께 사용 가능
- 사용자 친화성, 모듈성, 확장성을 통해 빠르고 간편한 프로토타이핑 가능
- 컨볼루션 신경망, 순환 신경망, 그리고 둘의 조합까지 모두 지원
- CPU와 GPU에서 매끄럽게 실행
"""

import numpy as np
from tensorflow import keras

tf.__version__
keras.__version__

"""레이어들을 import하는 방식 (1)
- 일일이 import 하지 않아도 됨

- 코드가 다소 길어질 수 있음
"""

keras.layers.Dense(10, activation = 'sigmoid')

keras.Model()

keras.models.Sequential()

"""레이어들을 import하는 방식 (2)
- 일일이 import 해야함
- 코드가 간결
"""

from tensorflow.keras.layers import Dense , Input, Flatten, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras import Model

Dense(10,activation='relu')

Flatten(input_shape = [28,28])

X_train = np.random.randn(5500, 2)

Input(shape=X_train.shape[1:])

"""## 주요 레이어

### Dense
- Fully-Connected Layer

- 노드수(유닛수), 활성화 함수 등을 지정

- name을 통한 레이어간 구분 가능

- 기본적으로  
  'Glorot_uniform' 가중치(Xavier 분포 초기화), zeros bias로 초기화
  - kernel_initializer 인자를 통해 다른 가중치 초기화를 진행할 수 있음
"""

dense = Dense(10, activation = 'relu', name = 'Dense Layer')

dense

dense2 = Dense(15, activation='softmax')

dense2

"""### Activation
- Dense layer에서 미리 활성화함수를 지정할 수도 있지만 때에 따라서 따로 레이어를 만들어줄 수 있음
"""

dense = Dense(10, kernel_initializer = 'he_normal', name = 'Dense Layer')
dense = Activation(dense)
dense

"""### Flatten
- 배치 크기(또는 데이터 크기)를 제외하고 데이터를 1차원으로 쭉 펼치는 작업

- 예시)
        (128, 3, 2, 2) ---> (128, 12)
      
"""

flatten  = Flatten(input_shape=(28, 28))
flatten

"""### Input 
- 모델의 입력을 정의

- shape, dtype을 포함

- 하나의 모델은 여러 개의 입력을 가질 수 있음

- summary() 메소드를 통해서는 보이지 않음
"""

input_1 = Input(shape=(28, 28), dtype=tf.float32)
input_2 = Input(shape=(8,), dtype=tf.float32)

input_1

input_2

"""## 모델 구성 방법
- Sequential()

- 서브클래싱(Subclassing)

- 함수형 API

### Sequential()

- 모델이 순차적으로 진행할 때 사용

- 간단한 방법
  - Sequential 객체 생성 후, **add**를 통한 방법
  
  - Sequential 인자에 한번에 추가

- 다중 입력 및 출력이 존재하는 등의 복잡한 모델을 구성할 수 없음
"""

from tensorflow.keras.layers import Dense, Input, Flatten
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import plot_model

model = Sequential()
model.add(Input(shape=(28,28)))
model.add(Dense(300, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(10, activation='softmax'))

"""모델 구조 확인
- model 객체의 **summary()** 이용
"""

model.summary()

plot_model(model)

plot_model(model, to_file='model1.png')

model = Sequential([Input(shape=(28,28), name="Input"),
                    Dense(300, activation = 'relu', name= 'Dense1'),
                    Dense(100, activation = 'relu', name= 'Dense2'),
                    Dense(10, activation ='softmax', name = 'output')])

model.summary()

plot_model(model)

plot_model(model, to_file='model2.png')

"""### 함수형 API

- 가장 권장되는 방법

- 모델을 복잡하고, 유연하게 구성 가능

- 다중 입출력을 다룰 수 있음
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras.utils import plot_model

inputs = Input(shape=(28,28,1))

x = Flatten(input_shape= (28,28,1))(inputs)
x = Dense(300, activation='relu')(x)
x = Dense(100, activation='relu')(x)
x = Dense(10, activation='softmax')(x)

model = Model(inputs = inputs, outputs=x)

model.summary()

plot_model(model)

plot_model(model, to_file='model3.png')

from tensorflow.keras.layers import Concatenate

# 리니어 관계를 밝혀줄수 있는 모델

input_layer = Input(shape=(28,28))
hidden1 = Dense(100, activation='relu')(input_layer)
hidden2 = Dense(30, activation='relu')(hidden1)
# 입력 목록을 연결하는 계층
concat = Concatenate()([input_layer, hidden2])
output = Dense(1)(concat)

model = Model(inputs=[input_layer], outputs=[output])

model.summary()

plot_model(model)

plot_model(model, to_file='model4.png')

input_1 = Input(shape=(10,10), name='input_1')
input_2 = Input(shape=(10,28), name='input_2')

hidden1 = Dense(100, activation='relu')(input_2)
hidden2 = Dense(10, activation='relu')(hidden1)
# 10,10 2개가 concat되서 10,20이 됨
concat = Concatenate()([input_1, hidden2])
output = Dense(1, activation = 'sigmoid', name='output')(concat)

model = Model(inputs=[input_1, input_2], outputs = [output])

model.summary()

plot_model(model)

plot_model(model, to_file='model_5.png')

input_ = Input(shape=(10,10), name='input_')

hidden1 = Dense(100, activation='relu')(input_)
hidden2 = Dense(10, activation='relu')(hidden1)

output = Dense(1, activation = 'sigmoid', name='main_output')(hidden2)
sub_out = Dense(1, name='sum_output')(hidden2)

model = Model(inputs=[input_], outputs=[output, sub_out])

model.summary()

plot_model(model)

plot_model(model, to_file='model6.png')

input_1 = Input(shape=(10,10), name='input_1')
input_2 = Input(shape=(10, 28), name = 'input_2')

hidden1 = Dense(100, activation='relu')(input_2)
hidden2 = Dense(10, activation='relu')(hidden1)
concat = Concatenate()([input_1, hidden2])
output = Dense(1, activation='sigmoid', name='main_output')(concat)
sub_out = Dense(1 , name='sum_output')(hidden2)

model = Model(inputs=[input_1, input_2], outputs=[output, sub_out])

model.summary()

plot_model(model)

plot_model(model, to_file='model7.png')

"""### 서브클래싱(Subclassing)
- 커스터마이징에 최적화된 방법

- Model 클래스를 상속받아 Model이 포함하는 기능을 사용할 수 있음  
  - `fit()`, `evaluate()`, `predict()`
  - `save()`, `load()`

- 주로 `call()` 메소드안에서 원하는 계산 가능
  - for, if, 저수준 연산 등

- 권장되는 방법은 아니지만 어떤 모델의 구현 코드를 참고할 때  
  해석할 수 있어야함
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras.utils import plot_model

class MyModel(Model):
  def __init__(self, unit=30, activation='relu', **kwargs):
    super(MyModel, self).__init__(**kwargs)

    self.dense_layer1 = Dense(300, activation=activation)
    self.dense_layer2 = Dense(100, activation=activation)
    self.dense_layer3 = Dense(units, activation=activation)

    self.output_layer = Dense(10, activation='softmax')

  def call(self, inputs):
    x = self.dense_layer1(inputs)
    x = self.dense_layer2(x)
    x = self.dense_layer3(x)
    x = self.output_layer(x)

    return x

"""## 모델 가중치 확인"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras.utils import plot_model

inputs = Input(shape=(28,28,1))

x = Flatten(input_shape=(28, 28, 1))(inputs)
x = Dense(300, activation = 'relu')(x)
x = Dense(100 ,activation='relu')(x)
x = Dense(10, activation='softmax')(x)

model = Model(inputs=inputs, outputs=x)

model.summary()

"""모델의 레이어들이 리스트로 표현됨

"""

model.layers

hidden_2 = model.layers[2]
hidden_2.name

"""위의 layer name 참고"""

model.get_layer('dense_23') is hidden_2

weights, biases = hidden_2.get_weights()

weights

biases

print(weights.shape)
print(biases.shape)

"""## 모델 컴파일(compile)

- 모델을 구성한 후, 사용할 손실함수(loss), 옵티마이저(optimizer)를 지정

"""

model.compile(loss='sparse_categorical_crossentropy',
              optimizer = 'sgd',
              metrics = ['accuracy'])

"""### 참고1

#### loss
- `keras.losses.sparse_categorical_crossentropy` 등과 같이 지정 가능
  - `sparse_categorical_crossentropy`  
    클래스가 배타적, 즉 MNIST 예제에서 (0, 1, 2, ..., 9)와 같은 방식으로 구분되어 있을 때 사용

  - `categorical_cross_entropy`
    클래스가 원-핫 인코딩 방식으로 되어 있을 때 사용

  - `binary_crossentropy`
  이진 분류를 수행할 때 사용


#### optimizer
- `keras.optimizer.SGD()` 등과 같이 사용 가능
  - 보통 옵티마이저의 튜닝을 위해 따로 객체를 생성하여 컴파일    
        optimizer = keras.optimizer.SGD(learning_rate=1e-5)  
        model.compile(...,
                      optimizer=optimzer,
                      ...)

- `keras.optimizer.Adam()` 도 많이 사용

#### metrics
- 모니터링할 지표

- 주로 'accuracy', 'acc'로도 가능

### 참고2 - 컴파일 설정 예시
- 평균 제곱 오차 회귀 문제  
      model.compile(loss = 'mse',  
                    optimizer = RMSprop(),  
                    metrics=['mae'])

- 이진 분류
      model.compile(loss = 'binary_crossentropy',  
                    optimizer = RMSprop(),  
                    metrics=['accuracy'])



- 다항 분류
      model.compile(loss = 'categorical_crossentropy',  
                    optimizer = RMSprop(),  
                    metrics=['accuracy'])

## 모델 훈련, 평가 및 예측
- `fit()` 함수

  - train_data(학습 데이터), train_label(데이터의 정답)
  
  - epochs
  
  - batch_size
  
  - validation_data

- `evaluate()` 함수
  - 테스트 데이터


- `predict()`
  임의의 데이터를 인자로 넣어 예측 가능

## MNIST 예제를 통해 모델 구성하기

- keras.datasets 에 포함되어 있는 데이터셋

  <img src="https://storage.googleapis.com/tfds-data/visualization/fig/mnist-3.0.1.png" width="400">

  <sub>출처: https://www.tensorflow.org/datasets/catalog/mnist</sub>

### modules import
"""

import tensorflow as tf
from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras.models import Sequential
from tensorflow.keras import models
from tensorflow.keras.layers import Dense, Input, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model

from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')

"""### 데이터셋 로드
- MNIST 데이터셋을 로드

- Train Data 중, 30%를 검증 데이터(validation data)로 사용

"""

tf.random.set_seed(111)

(x_train_full, y_train_full), (x_test, y_test) = load_data(path='mnist.npz')

x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full,
                                                  test_size=0.3, # 30%를 validation을 뺀것
                                                  random_state=111)

"""### 데이터 확인"""

num_x_train = (x_train.shape[0])
num_x_val = (x_val.shape[0])
num_x_test = (x_test.shape[0])

print("trian data : {}\t lable: {}".format(x_train_full.shape, y_train_full.shape))
print("trian data : {}\t lable: {}".format(x_train.shape, y_train.shape))
print("validation data : {}\t lable: {}".format(x_val.shape, y_val.shape))
print("test data : {}\t lable: {}".format(x_test.shape, y_test.shape))

num_sample = 5

random_idxs = np.random.randint(60000, size=num_sample)

plt.figure(figsize=(14,8))

for i, idx in enumerate(random_idxs):
  img = x_train_full[idx, :]
  label = y_train_full[idx]

  plt.subplot(1, len(random_idxs), i+1)
  plt.imshow(img)
  plt.title("Index : {}, Label: {}".format(idx,label))

y_train[0]

"""### 데이터 전처리
- Normalization
"""





"""### 모델 구성 (Sequential)"""









"""### 모델 컴파일"""



"""### 모델 학습
- 모델 시각화를 위해 history라는 변수에 학습 과정을 담음
"""



"""### 학습 결과 시각화"""





"""### 모델 평가 (1)
- evaluate
"""



"""### 학습된 모델을 통해 값 예측"""





"""### 모델 평가 (2)
- 혼동행렬 (Confusion Maxtrix)
"""





"""### 모델 평가 (3)
- 분류 보고서
"""



"""## 모델 저장과 복원
- `save()`

- `load_model()`

- (주의)  
  시퀀셜 API, 함수형 API에서는 **모델**의 저장 및 로드기 가능하지만 서브클래싱 방식으로는 할 수 없음  
  
  서브클래싱 방식
      save_weights()
      load_weights()  
  위 두가지를 통해 **모델의 파라미터**만 저장 및 로드


- JSON 형식
  - `model.to_json()` (저장)

  - `tf.keras.models.model_from_json(file_path)` (복원)

- YAML로 직렬화
  - `model.to_yaml()` (저장)

  - `tf.keras.models.model_from_yaml(file_path)` (복원)

"""











"""## 콜백 (Callbacks)

- `fit()` 함수의 callbacks 매개변수를 사용하여 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트를 지정할 수 있음

- 여러 개 사용 가능

- ModelCheckpoint
  - `tf.keras.callbacks.ModelCheckpoint`
  - 정기적으로 모델의 체크포인트를 저장하고, 문제가 발생할 때 복구하는데 사용

- EarlyStopping
  - `tf.keras.callbacks.EarlyStopping`
  - 검증 성능이 한동안 개선되지 않을 경우 학습을 중단할 때 사용

- LearningRateSchduler
  - `tf.keras.callbacks.LearningRateSchduler`
  - 최적화를 하는 동안 학습률(learning_rate)를 동적으로 변경할 때 사용

- TensorBoard
  - `tf.keras.callbacks.TensorBoard`
  - 모델의 경과를 모니터링할 때 사용

"""







"""### ModelCheckpoint"""







"""최상의 모델만을 저장
- save_best_only=True
"""







"""### EarlyStopping
- 일정 에포크(patience) 동안 검증 세트에 대한 점수가 오르지 않으면 학습을 멈춤

- 모델이 향상되지 않으면 학습이 자동으로 중지되므로,  
  에포크(epochs) 숫자를 크게 해도 무방

- 학습이 끝난 후의 최상의 가중치를 복원하기 때문에 모델을 따로 복원할 필요없음
"""



"""### LearningRateScheduler"""







"""### Tensorboard
- 텐서보드를 이용하여 학습과정 모니터링

텐서보드를 사용하기 위해 logs폴더를 만들고, 학습이 진행되는 동안 로그 파일을 생성
"""







"""load하는데 시간 소요

- load가 안된다면 port번호를 바꿔서 실행!  
  ex) %tensorboard --logdir {log_dir} port 8000
"""

