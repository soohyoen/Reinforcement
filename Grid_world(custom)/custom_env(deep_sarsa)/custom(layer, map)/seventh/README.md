발산까지는 막았고, layer에 Input되는 통로의 값을 줄여보고 싶음 -> 학습속도가 빠르지 않을까..

Dense: 출력 뉴런의 수 60-> 40

현재 입력 뉴런은 39개 

→ 발산은 되지 않지만 학습이 진행이 되지 않음.. step 하나당 10000개가 넘어버림

→ 학습 결과는 좋음 하지만 val env로 검증 시 전혀 학습이 안된것 처럼 보임.
   과적합으로 예상되며, Dropout의 기법을 적용해봄
   Dropout을 적용해도 여전히 도리도리... 과적합의 문제가 아닌거같은데
  
-> 아마도 학습이 편향된 결과이기도 하고 학습패턴이 제한되어 나온 결과라 생각함.
-> 환경을 변화해가며 학습을 시도해야할 듯 -> 못해도 10개의 환경변화는 고려해보자  

-> 환경을 1000 epoch당 바꿔주는 방법을 고려하여 코드를 짬 근데 수렴이 안됨 처음에
-> 그래서 activation function하고 loss function을 다시  linear하고 mse로 바꿔줬더니 수렴함..
-> 만약 이전과 같이 발산한다면 fine tunning 시도
-> epoch 967에서 아예 멈춰버림, 적당선 600번에서 멈춰야할듯 아니면 시간변수를 투입하거나 step이 500번 넘어가면 버리는거 고려해야할 듯
-> 한 Env당 Local step이 500넘어가면 done
-> local step으로 규제해도 500이 넘어가는 값이 학습이 되기때문에 결과적으로 이상해짐..
-> epoch를 650으로 변경
-> 650선이 과적합 안나고 괜찮은것같음.. 근데 _tkinter.TclError: image "pyimage4" doesn't exist 에러 발생
-> tkinter를 초기화하기위해 호출되며 한번만 호출 가능함.

-> 결과적으로 학습시킨 weghit를 돌렸을 때, 학습이 안된것처럼 나옴.
-> 그래서 parameter값을 늘려보고(Dense 추가) 일단은 env 10개로 진행
-> global_step이 다른 env에도 적용되는듯..? 왜그런거지..
-> local_step, global_step 말고 early_stop 기능을 추가시켜야 할듯
