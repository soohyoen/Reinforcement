{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "149080a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. 지도는 5x525개의 위치로 이어지는 그리드 월드입니다.\\n2. 알파벳 R, G, B, Y는 4자리입니다.\\n3. 승객은 4개 위치 중 아무 곳에나 있을 수 있습니다.\\n4. 승객의 목적지는 왼쪽 3개 위치 중 하나일 수 있습니다.\\n5. 파이프 기호 |는 벽을 나타냅니다. 즉, |Y|x:위치 에 있으면 왼쪽으로 이동 x하여 위치에 도달할 수 없습니다 .Y\\n6. 콜론 기호 :는 패스를 나타냅니다. 즉, 위치에 |B:a|있는 경우 a왼쪽으로 이동하여 위치 B에 도달할 수 있습니다.\\n7. 택시는 통과할 수 :있지만 통과할 수 없습니다 .|\\n8. 20환경 은 승객이 목적지까지 내려갈 때 포인트를 보상 합니다.\\n9. -10승객이 없는 셀에서 픽업 작업을 수행하면 환경 에서 점수를 얻습니다.\\n10. -10승객이 택시에 탑승하지 않은 상태에서 하차 작업을 수행하면 환경에서 벌점 을 받습니다.\\n11. -1환경 은 다른 모든 행동에 불이익 을 줍니다.\\n12. 이 환경에는 500개의 상태가 있습니다.\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. 지도는 5x525개의 위치로 이어지는 그리드 월드입니다.\n",
    "2. 알파벳 R, G, B, Y는 4자리입니다.\n",
    "3. 승객은 4개 위치 중 아무 곳에나 있을 수 있습니다.\n",
    "4. 승객의 목적지는 왼쪽 3개 위치 중 하나일 수 있습니다.\n",
    "5. 파이프 기호 |는 벽을 나타냅니다. 즉, |Y|x:위치 에 있으면 왼쪽으로 이동 x하여 위치에 도달할 수 없습니다 .Y\n",
    "6. 콜론 기호 :는 패스를 나타냅니다. 즉, 위치에 |B:a|있는 경우 a왼쪽으로 이동하여 위치 B에 도달할 수 있습니다.\n",
    "7. 택시는 통과할 수 :있지만 통과할 수 없습니다 .|\n",
    "8. 20환경 은 승객이 목적지까지 내려갈 때 포인트를 보상 합니다.\n",
    "9. -10승객이 없는 셀에서 픽업 작업을 수행하면 환경 에서 점수를 얻습니다.\n",
    "10. -10승객이 택시에 탑승하지 않은 상태에서 하차 작업을 수행하면 환경에서 벌점 을 받습니다.\n",
    "11. -1환경 은 다른 모든 행동에 불이익 을 줍니다.\n",
    "12. 이 환경에는 500개의 상태가 있습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9c6ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86ccc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q, s, eps=0.1):\n",
    "    \n",
    "    # uniform → 균등 분포 정보를 생성\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    \n",
    "    else:\n",
    "        return greedy(Q, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4560bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축을 따라 최대값의 인덱스를 반환함\n",
    "def greedy(Q, s):\n",
    "    return np.argmax(Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d103d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(env, Q, num_episodes=100, to_print=False):\n",
    "    \n",
    "    tot_rew= []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        game_rew = 0\n",
    "        \n",
    "        while not done:\n",
    "            next_state, rew, done, _ = env.step(greedy(Q, state))\n",
    "            \n",
    "            state = next_state\n",
    "            game_rew += rew\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                tot_rew.append(game_rew)\n",
    "                \n",
    "    if to_print:\n",
    "        print('Mean score: %.3f of %i games!'%(np.mean(tot_rew), num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a27266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma= 0.95, eps_decay= 0.00005):\n",
    "    \n",
    "    # action의 갯수\n",
    "    nA = env.action_space.n\n",
    "    # 차원의 수\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    Q = np.zeros((nS, nA))\n",
    "    games_reward = []\n",
    "    test_rewards = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        tot_rew = 0\n",
    "        \n",
    "        # epsilon이 0.01에 닿을때까지 감쇠시킴\n",
    "        if eps > 0.01:\n",
    "            eps -= eps_decay\n",
    "            \n",
    "        while not done:\n",
    "            \n",
    "            action = eps_greedy(Q, state, eps)\n",
    "            next_state, rew, done, _, info = env.step(action)\n",
    "            # 상태- 동작 값 업데이트(다음 상태에 대한 최대 Q값 가져오기)\n",
    "            Q[state][action] = Q[state][action] + lr * (rew + gamma * np.argmax(Q[next_state]) - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            tot_rew += rew\n",
    "            if done:\n",
    "                games_reward.apped(tot_rew)\n",
    "        \n",
    "        # 300번의 에피소드마다 점검함        \n",
    "        if (ep % 300) == 0:\n",
    "            test_rew = run_episodes(env, Q, 1000)\n",
    "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
    "            test_rewards.append(test_rew)\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8282ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on-policy 알고리즘으로 Q-learning과 차별화되는 영역 중 하나\n",
    "# 에이전트가 행동하고 가치 함수를 업데이트하는 데 동일한 정책을 사용함\n",
    "# 정책 외 접근 방식에는 행동 및 업데이트에 대해 서로 다른 정책을 사용\n",
    "\n",
    "def SARSA(env, lr=0.01, num_episodes =10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    Q = np.zeros((nS, nA))\n",
    "    games_reward = []\n",
    "    test_rewards = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        tot_rew = 0\n",
    "        \n",
    "        if eps > 0.01:\n",
    "            eps -= eps_decay\n",
    "            \n",
    "        action =eps_greedy(Q, state, eps)\n",
    "        \n",
    "        # main body의 환경이 끝날때까지 loop\n",
    "        while not done:\n",
    "            next_state, rew, done, _, info = env.step(action)\n",
    "            \n",
    "            next_action = eps_greedy(Q, next_state, eps)\n",
    "            \n",
    "            Q[state][action] = Q[state][action] + lr * (rew+gamma * Q[next_state][next_action] - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            tot_rew += rew\n",
    "            \n",
    "            if done:\n",
    "                games_reward.append(tot_rew)\n",
    "                \n",
    "        if (ep % 300) == 0:\n",
    "            test_rew = run_episodes(env, Q, 1000)\n",
    "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
    "            test_rewards.append(test_rew)\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "131a8daa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3537/72690569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Taxi-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mQ_qlearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mQ_sarsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSARSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3537/1747445215.py\u001b[0m in \u001b[0;36mQ_learning\u001b[0;34m(env, lr, num_episodes, eps, gamma, eps_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 상태- 동작 값 업데이트(다음 상태에 대한 최대 Q값 가져오기)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3537/1631479524.py\u001b[0m in \u001b[0;36meps_greedy\u001b[0;34m(Q, s, eps)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3537/2760471663.py\u001b[0m in \u001b[0;36mgreedy\u001b[0;34m(Q, s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 축을 따라 최대값의 인덱스를 반환함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('Taxi-v3')\n",
    "    \n",
    "    Q_qlearning = Q_learning(env, lr=0.1, num_episodes =5000, eps=0.4, gamma = 0.95, eps_decay = 0.001)\n",
    "    \n",
    "    Q_sarsa = SARSA(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0a9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein_study",
   "language": "python",
   "name": "rein_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
